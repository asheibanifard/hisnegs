\documentclass[11pt,a4paper]{article}

% ── Packages ──────────────────────────────────────────────────────────────────
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{float}
\usepackage{cleveref}        % smart cross-references
\usepackage{siunitx}         % consistent unit formatting
\usepackage{array}           % extended table column specs
\usepackage{multirow}        % multirow table cells
\usepackage{nicefrac}        % compact in-line fractions

% ── Theorem environments ──────────────────────────────────────────────────────
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

% ── Custom colours ────────────────────────────────────────────────────────────
\definecolor{myblue}{RGB}{31,78,121}
\hypersetup{colorlinks=true,linkcolor=myblue,citecolor=myblue,urlcolor=myblue}

% ── Notation macros ───────────────────────────────────────────────────────────
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}
\newcommand{\bR}{\mathbf{R}}
\newcommand{\bT}{\mathbf{T}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bq}{\mathbf{q}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bJ}{\mathbf{J}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Gcal}{\mathcal{G}}
\newcommand{\Real}{\mathbb{R}}

% ── Title ─────────────────────────────────────────────────────────────────────
\title{%
  \textbf{Real-Time Maximum Intensity Projection Rendering\\
  for 3D Fluorescence Microscopy\\
  via Gaussian Splatting}\\[6pt]
  \large Technical Report: Implementation, Training, and Evaluation
}
\author{%
  HiSNeGS --- Hierarchical Sparse Neural Gaussian Splatting
}
\date{\today}

% ──────────────────────────────────────────────────────────────────────────────
\begin{document}
% ──────────────────────────────────────────────────────────────────────────────

\maketitle

\begin{abstract}
We present MIP Gaussian Splatting, a real-time rendering method for
three-dimensional fluorescence microscopy volumes.  The method represents
neurite structures as a compact set of anisotropic 3-D Gaussian primitives and
renders Maximum Intensity Projections (MIPs) through a fully differentiable soft
splatting pipeline.  We document the complete system from ground-truth
projection generation through multi-stage training to extensive quantitative
evaluation.  Key results: the renderer achieves $>30$ frames per second (FPS) at
$256{\times}256$ pixel resolution with a mean PSNR above 33 dB, while
compressing a $100{\times}647{\times}813$ fluorescence volume by more than
$400{\times}$ relative to uncompressed storage.  We provide a thorough
exposition of every design choice, including a multi-component loss function
(weighted MSE, SSIM, edge, and intensity-distribution terms), an online soft-max
splatting CUDA kernel, and an adaptive Gaussian density controller.  An ablation
study with paired $t$-tests confirms the statistical significance of each loss
component.
\end{abstract}

\tableofcontents
\newpage

% ══════════════════════════════════════════════════════════════════════════════
\section{Introduction}
\label{sec:intro}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Problem Statement}

Fluorescence microscopy produces high-resolution three-dimensional volumetric
images of biological specimens such as neurons.  Interactive, real-time
visualization of these volumes is indispensable for scientific analysis, yet
conventional volumetric ray-marching remains computationally expensive, typically
achieving only 1--5 FPS at $256^3$ voxel resolution on modern GPUs.  This
bottleneck prevents interactive exploration and hinders downstream quantitative
analysis.

\subsection{Our Approach}

We propose \textbf{MIP Gaussian Splatting} to overcome this limitation.  The key
ideas are:
\begin{enumerate}
    \item Represent the 3-D volume as a compact set of $K \approx 50{,}000$
          anisotropic Gaussian primitives (a \emph{Gaussian Mixture Field}, GMF).
    \item Render Maximum Intensity Projections via efficient 2-D differentiable
          Gaussian splatting rather than ray marching.
    \item Optimize primitive parameters end-to-end against multi-view MIP ground
          truth using a composite perceptual loss.
\end{enumerate}
The resulting renderer achieves $>30$ FPS at $256^2$, a $>100\times$ speedup
over ray marching, while maintaining reconstruction quality above 33 dB PSNR and
compressing the raw volume by up to $401\times$.

\subsection{Pipeline Overview}

The complete system consists of five sequential stages:
\begin{enumerate}
    \item \textbf{Volume Preprocessing:} Load the fluorescence TIFF stack and
          compute an aspect-ratio correction tensor to account for anisotropic
          voxel spacing.
    \item \textbf{Ground Truth Generation:} Render 106 multi-view MIP projections
          by volumetric ray marching.
    \item \textbf{GMF Fitting (Stage~1):} Fit a 3-D Gaussian Mixture Field to the
          volume by minimizing a 3-D reconstruction loss on sampled points.
    \item \textbf{MIP Splatting Training (Stage~2):} Optimize the GMF parameters
          for 2-D projection quality using the composite perceptual loss defined
          in \cref{sec:loss}.
    \item \textbf{Evaluation:} Benchmark rendering performance, visual quality,
          and model efficiency.
\end{enumerate}

% ══════════════════════════════════════════════════════════════════════════════
\section{Mathematical Formulation}
\label{sec:math}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Gaussian Mixture Field Representation}
\label{sec:gmf}

\begin{definition}[Gaussian Mixture Field]
A \emph{Gaussian Mixture Field} (GMF) approximates the fluorescence intensity
function $f : \Real^3 \to [0,1]$ of a volumetric specimen as a weighted sum of
$K$ anisotropic Gaussian basis functions:
\begin{equation}
  f(\bx) = \sum_{k=1}^{K} a_k \,
            \exp\!\left(-\tfrac{1}{2}\,
            (\bx - \bmu_k)^\top \bSigma_k^{-1} (\bx - \bmu_k)
            \right),
  \label{eq:gmf}
\end{equation}
where the parameters of the $k$-th primitive $\Gcal_k$ are defined below.
\end{definition}

\paragraph{Notation and parameter definitions.}
Each Gaussian primitive $\Gcal_k$ is uniquely characterized by three groups of
learnable parameters.

\begin{enumerate}
\item \textbf{Center} $\bmu_k \in \Real^3$: the mean position of the primitive
      in a normalized world coordinate system $[-1,1]^3$.  During optimization
      $\bmu_k$ is updated directly via gradient descent.

\item \textbf{Covariance} $\bSigma_k \in \Real^{3\times3}$: a symmetric
      positive-definite matrix that controls the shape and orientation of the
      primitive.  To guarantee positive definiteness and to separate rotation from
      scale, $\bSigma_k$ is factored as
      \begin{equation}
        \bSigma_k = \bR_k \,\operatorname{diag}(\bs_k^{\odot 2})\, \bR_k^\top,
        \label{eq:covariance}
      \end{equation}
      where:
      \begin{itemize}
        \item $\bR_k \in \mathrm{SO}(3)$ is a rotation matrix constructed from
              a unit quaternion $\bq_k \in \Real^4$,
              $\|\bq_k\|_2 = 1$, via the standard Rodrigues formula.
        \item $\bs_k = \exp(\tilde{\bs}_k) \in \Real_{>0}^3$ are the
              anisotropic half-widths (scales) along the three principal axes of
              the primitive.  The raw log-scales $\tilde{\bs}_k \in \Real^3$ are
              the actual optimized variables; exponentiation enforces positivity
              without requiring constrained optimization.
        \item $\bs_k^{\odot 2}$ denotes element-wise squaring of $\bs_k$, so
              $\operatorname{diag}(\bs_k^{\odot 2})$ yields the diagonal
              variance matrix.
      \end{itemize}

\item \textbf{Intensity} $a_k \in (0,1)$: the peak emission amplitude of the
      primitive, modelling the local fluorescence brightness.  It is
      parameterized as $a_k = \sigma(\ell_k)$, where
      $\sigma(z) = (1+e^{-z})^{-1}$ is the logistic sigmoid and
      $\ell_k \in \Real$ is the unconstrained logit.  This replaces the
      separate opacity and spherical-harmonic color coefficients used in
      3D Gaussian Splatting (3DGS)~\cite{kerbl2023gaussians} for RGB scenes,
      reflecting the additive, occlusion-free physics of fluorescence emission.
\end{enumerate}

\begin{remark}
The factorization in \cref{eq:covariance} is the same as that used in
3DGS~\cite{kerbl2023gaussians}, but here we restrict to a single scalar
intensity instead of per-primitive color, reducing the parameter count per
primitive from $3{+}4{+}3{+}48 = 58$ (3DGS) to $3{+}4{+}3{+}1 = 11$.
\end{remark}

\subsection{MIP Splatting: Rendering Pipeline}
\label{sec:render}

Given camera extrinsics $(\bR_c, \bT_c)$ (rotation matrix and translation
vector defining the world-to-camera transform), we compute the MIP-splatted
image $I \in [0,1]^{H \times W}$ in four steps.

\subsubsection{Step 1 — World-to-Camera Transform}

Each primitive center and covariance are mapped into camera space:
\begin{align}
  \bmu_k^{\mathrm{cam}} &= \bR_c\,\bmu_k + \bT_c,
  \label{eq:w2c_mu}\\
  \bSigma_k^{\mathrm{cam}} &= \bR_c\,\bSigma_k\,\bR_c^\top.
  \label{eq:w2c_sigma}
\end{align}
Here $\bmu_k^{\mathrm{cam}} = (x_k, y_k, z_k)^\top$ gives the camera-frame
position, with $z_k > 0$ denoting depth in front of the camera.

\subsubsection{Step 2 — Perspective Projection via EWA Splatting}

We adopt the Elliptical Weighted Average (EWA) splatting approximation
\cite{zwicker2002ewa} to project the 3-D Gaussians onto the image plane.
The first-order Jacobian of the pinhole projection map at depth $z_k$ is:
\begin{equation}
  \bJ_k = \begin{pmatrix}
    f_x / z_k & 0 & -f_x\, x_k / z_k^2 \\
    0 & f_y / z_k & -f_y\, y_k / z_k^2
  \end{pmatrix} \in \Real^{2\times3},
  \label{eq:jacobian}
\end{equation}
where:
\begin{itemize}
  \item $f_x, f_y > 0$: focal lengths in pixels along the horizontal and
        vertical axes, derived from the field-of-view angle
        $\alpha = 50^\circ$ as $f_x = f_y = \tfrac{W}{2\tan(\alpha/2)}$.
  \item $x_k, y_k$: horizontal and vertical coordinates of
        $\bmu_k^{\mathrm{cam}}$ in camera space.
  \item $z_k$: depth (distance from camera along the optical axis) of
        $\bmu_k^{\mathrm{cam}}$.
\end{itemize}

The projected 2-D parameters are then:
\begin{align}
  \bmu_k^{\mathrm{2D}} &= \begin{pmatrix}
    f_x\, x_k / z_k + c_x \\
    f_y\, y_k / z_k + c_y
  \end{pmatrix} \in \Real^2,
  \label{eq:proj_mu}\\[4pt]
  \bSigma_k^{\mathrm{2D}} &= \bJ_k\,\bSigma_k^{\mathrm{cam}}\,\bJ_k^\top
                              \in \Real^{2\times2},
  \label{eq:proj_sigma}
\end{align}
where $(c_x, c_y)$ is the principal point (image center).

\subsubsection{Step 3 — 2-D Gaussian Evaluation}

For each pixel at position $\bp = (u, v)^\top \in \Real^2$, the contribution of
primitive $k$ is evaluated as the 2-D Gaussian:
\begin{equation}
  \Gcal_k^{\mathrm{2D}}(u, v) =
    \exp\!\left(-\tfrac{1}{2}\,
      (\bp - \bmu_k^{\mathrm{2D}})^\top
      \bigl(\bSigma_k^{\mathrm{2D}}\bigr)^{-1}
      (\bp - \bmu_k^{\mathrm{2D}})
    \right) \in [0, 1].
  \label{eq:gauss2d}
\end{equation}
Primitives whose Mahalanobis distance $(\bp - \bmu_k^{\mathrm{2D}})^\top
(\bSigma_k^{\mathrm{2D}})^{-1}(\bp - \bmu_k^{\mathrm{2D}}) > \delta^2$
(with $\delta = 4$) are culled before evaluation to reduce computation.

The \emph{effective contribution} of primitive $k$ at pixel $(u,v)$ combines
intensity and spatial footprint:
\begin{equation}
  g_k(u,v) = a_k \cdot \Gcal_k^{\mathrm{2D}}(u,v) \in [0, 1].
  \label{eq:gk}
\end{equation}

\subsubsection{Step 4 — Differentiable Soft-MIP Aggregation}
\label{sec:softmip}

The hard Maximum Intensity Projection is
$I_{\mathrm{hard}}(u,v) = \max_{k}\, g_k(u,v)$,
which is non-differentiable with respect to all Gaussian parameters other than
the maximizing primitive.  To enable gradient flow through \emph{all} primitives,
we substitute the hard max with a temperature-scaled soft-max:

\begin{equation}
  I(u,v) = \sum_{k=1}^{K}
    \underbrace{%
      \frac{\exp\!\bigl(\beta\, g_k(u,v)\bigr)}
           {\displaystyle\sum_{j=1}^{K}\exp\!\bigl(\beta\, g_j(u,v)\bigr)}
    }_{\textstyle w_k^{\mathrm{soft}}(u,v)}
    \cdot g_k(u,v),
  \label{eq:softmip}
\end{equation}
where:
\begin{itemize}
  \item $\beta > 0$ is the \emph{temperature} (sharpness) parameter of the
        soft-max.  As $\beta \to \infty$, $w_k^{\mathrm{soft}} \to \mathbf{1}[k
        = \arg\max_j g_j]$ and \cref{eq:softmip} recovers the hard MIP exactly.
  \item In practice we use $\beta = 50$ at convergence, chosen to balance
        differentiability with approximation quality (PSNR $\approx 45$ dB
        relative to hard MIP; see \cref{tab:beta}).
  \item During training $\beta$ is linearly warmed up from 10 to 50 over the
        first 500 epochs to avoid sharp gradients early in optimization.
\end{itemize}

\begin{remark}[Numerical stability]
Direct evaluation of \cref{eq:softmip} is numerically unstable for large
$\beta$ because the exponentials may overflow.  We therefore implement an
\emph{online soft-max} (see \cref{sec:cuda}) using the standard log-sum-exp
trick: subtracting the running maximum $m = \max_j \beta g_j$ before
exponentiation ensures all terms lie in $(-\infty, 0]$, producing finite
results in single-precision arithmetic.
\end{remark}

% ══════════════════════════════════════════════════════════════════════════════
\section{Implementation Details}
\label{sec:impl}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Volume Preprocessing}

\subsubsection{Data Format and Normalization}
\begin{itemize}
  \item \textbf{Input format:} Multi-frame TIFF stack with spatial dimensions
        $(Z, Y, X) = (100, 647, 813)$ voxels, where $Z$ is the axial (depth)
        dimension and $X,Y$ are lateral dimensions.
  \item \textbf{Voxel spacing:} Anisotropic, with typical lateral resolution
        $\Delta_{XY} = 0.2\,\si{\micro\metre}$ and axial resolution
        $\Delta_Z = 1.0\,\si{\micro\metre}$, giving an axial-to-lateral
        anisotropy factor of $5\times$.
  \item \textbf{Intensity normalization:} Raw fluorescence intensities
        (typically 12- or 16-bit) are linearly rescaled to $[0, 1]$ via
        $v \leftarrow (v - v_{\min}) / (v_{\max} - v_{\min})$.
\end{itemize}

\subsubsection{Aspect-Ratio Correction}

Because voxels are anisotropic, a Gaussian primitive that appears isotropic in
voxel coordinates would represent a physically prolate or oblate structure.  We
correct for this by rescaling the normalized world coordinate system:
\begin{equation}
  \mathbf{s}_{\mathrm{asp}} =
    \frac{(1,\; Y/X,\; Z/X)}
         {\max(1,\; Y/X,\; Z/X)},
  \label{eq:aspect}
\end{equation}
so that all three axes are bounded in $[0, 1]$.  Every primitive center
$\bmu_k$ and scale $\bs_k$ is multiplied element-wise by
$\mathbf{s}_{\mathrm{asp}}$ before splatting, ensuring that Gaussians correctly
span physically equal distances along each axis.

\subsection{Configuration Parameters}

\Cref{tab:config} lists all key hyperparameters used in training and evaluation.

\begin{table}[H]
\centering
\caption{Key configuration parameters for training and rendering.}
\label{tab:config}
\begin{tabular}{@{}l l p{6cm}@{}}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Description} \\
\midrule
\multicolumn{3}{l}{\textit{Camera intrinsics}} \\
Horizontal FOV $\alpha$ & $50^\circ$ & Total horizontal field of view \\
Near plane & 0.01 & Minimum ray depth (normalized units) \\
Far plane & 10.0 & Maximum ray depth (normalized units) \\
Training resolution & $256{\times}256$ & Image size during optimization \\
\midrule
\multicolumn{3}{l}{\textit{Ground-truth ray marching}} \\
Samples per ray $N_s$ & 200 & Number of quadrature points \\
Integration interval & $[0.5,\;6.0]$ & Near/far bounds (normalized units) \\
\midrule
\multicolumn{3}{l}{\textit{Soft-MIP splatting}} \\
Temperature $\beta$ & 50 & Final soft-max sharpness \\
$\beta$ warm-up & $10 \to 50$ & Linear increase, epochs 1--500 \\
Mahalanobis cutoff $\delta$ & 4 & Culling threshold ($\delta^2=16$) \\
Splatting chunk size & 4096 & Primitives processed per CUDA tile \\
\midrule
\multicolumn{3}{l}{\textit{Optimization}} \\
Total epochs & 2000 & Full passes over the 106-view training set \\
Initial learning rate $\eta_0$ & $3\times10^{-3}$ & Adam optimizer \\
Final learning rate $\eta_T$ & $1\times10^{-5}$ & Cosine-annealed target \\
Foreground weight $w_{\mathrm{fg}}$ & 5.0 & Pixel reweighting for WMSE \\
Checkpoint interval & 100 & Epochs between model saves \\
\bottomrule
\end{tabular}
\end{table}

% ══════════════════════════════════════════════════════════════════════════════
\section{Stage 1: Ground-Truth MIP Generation}
\label{sec:gt}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Multi-View Camera Pose Generation}

We tile the viewing hemisphere with orbital cameras to produce a diverse
training set.  The camera center is placed at:
\begin{equation}
  \bT_c = r
    \begin{pmatrix}
      \sin\theta \cos\phi \\
      \sin\theta \sin\phi \\
      \cos\theta
    \end{pmatrix},
  \label{eq:orbit}
\end{equation}
where:
\begin{itemize}
  \item $r = 2.5$ (normalized units): orbital radius, chosen so that the
        entire normalized volume fits within the camera frustum.
  \item $\theta \in \{-30^\circ, 0^\circ, 30^\circ, 60^\circ\}$: elevation
        angle measured from the positive $z$-axis (zenith).
  \item $\phi \in [0^\circ, 360^\circ)$: azimuth angle, sampled at
        evenly-spaced intervals to produce 106 total views.
\end{itemize}
The camera always looks toward the origin, and the rotation matrix $\bR_c$ is
computed from the look-at direction and a fixed up-vector.

\subsection{Volumetric Ray Marching}

For each camera pose the reference MIP is computed by classical ray marching:

\begin{algorithm}[H]
\caption{Reference MIP via volumetric ray marching}
\label{alg:raymarch}
\begin{algorithmic}[1]
\FOR{each pixel $(u, v)$ in the $H{\times}W$ image}
    \STATE Compute ray origin $\mathbf{o}$ and unit direction $\mathbf{d}$
           from camera intrinsics and $(u,v)$
    \STATE $I_{\max} \gets 0$
    \FOR{sample index $i = 1, \ldots, N_s = 200$}
        \STATE $t_i \gets t_{\mathrm{near}} +
               (i-1)\,\tfrac{t_{\mathrm{far}} - t_{\mathrm{near}}}{N_s - 1}$
        \STATE $\bx_i \gets \mathbf{o} + t_i\,\mathbf{d}$
               \hfill(world-space sample point)
        \STATE $\bx'_i \gets \bx_i \odot \mathbf{s}_{\mathrm{asp}}$
               \hfill(apply aspect-ratio correction per \cref{eq:aspect})
        \STATE $v_i \gets \mathrm{trilinear}(V,\, \bx'_i)$
               \hfill(interpolate volume $V$ at $\bx'_i$)
        \STATE $I_{\max} \gets \max(I_{\max},\, v_i)$
    \ENDFOR
    \STATE $I(u, v) \gets I_{\max}$
\ENDFOR
\end{algorithmic}
\end{algorithm}

The total computational cost is approximately $N_s \times H \times W = 200
\times 256 \times 256 \approx 13.1\times10^6$ volume lookups per frame,
producing one ground-truth image in 200--500 ms.

\begin{table}[H]
\centering
\caption{Ground-truth dataset statistics.}
\label{tab:gt}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Number of views & 106 \\
Image resolution & $256{\times}256$ \\
Intensity range & $[0, 1]$ \\
Storage (FP32) & $\approx$27 MB \\
Total generation time & $\approx$50 s \\
\bottomrule
\end{tabular}
\end{table}

% ══════════════════════════════════════════════════════════════════════════════
\section{Stage 2: Gaussian Mixture Field Initialization}
\label{sec:gmf_init}
% ══════════════════════════════════════════════════════════════════════════════

Before 2-D MIP training, we initialize Gaussian primitives by fitting the GMF
directly to the 3-D volume.  This provides a semantically meaningful starting
point compared with random or grid initialization, and reduces the number of
epochs required for convergence in Stage~3.

\subsection{GMF Fitting}

Starting from $K_0$ Gaussians placed on a regular grid within the volume bounding
box, we minimize the mean squared volumetric reconstruction error:
\begin{equation}
  \Lcal_{\mathrm{3D}} = \frac{1}{|\mathcal{S}|}
    \sum_{\bx \in \mathcal{S}} \bigl(f(\bx) - V(\bx)\bigr)^2,
  \label{eq:loss3d}
\end{equation}
where $V(\bx) \in [0,1]$ is the trilinearly-interpolated ground-truth volume
intensity at position $\bx$, $f(\bx)$ is the GMF prediction from
\cref{eq:gmf}, and $\mathcal{S}$ is a randomly drawn mini-batch of 3-D sample
points.  After fitting, adaptive density control (split/clone/prune) is applied
to concentrate primitives in high-intensity regions (see \cref{sec:density}).

\subsection{Initialization Statistics}

\begin{table}[H]
\centering
\caption{GMF checkpoint used to initialize Stage-2 training.}
\label{tab:init}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Property} & \textbf{Value} \\
\midrule
Checkpoint file & \texttt{gmf\_refined\_best.pt} \\
Number of Gaussians $K$ & 12{,}145 \\
Memory footprint & 0.15 MB \\
Mean scale $(s^x, s^y, s^z)$ & $(0.031,\;0.028,\;0.089)$ \\
Intensity range $[a_{\min}, a_{\max}]$ & $[0.001,\;0.998]$ \\
\bottomrule
\end{tabular}
\end{table}

The elongated mean axial scale ($s^z \approx 0.089 \gg s^x,s^y \approx 0.030$)
reflects the $5\times$ voxel anisotropy: primitives must span more normalized
distance along $z$ to represent the same physical extent.

% ══════════════════════════════════════════════════════════════════════════════
\section{Stage 3: MIP Splatting Training}
\label{sec:training}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Training Objective}
\label{sec:loss}

We optimize all Gaussian parameters $\{\bmu_k, \tilde{\bs}_k, \bq_k, \ell_k\}_{k=1}^K$
by minimizing a composite perceptual loss that evaluates agreement between the
splatted prediction $P \in [0,1]^{H\times W}$ and the ground-truth MIP
$G \in [0,1]^{H\times W}$:

\begin{equation}
  \Lcal_{\mathrm{total}} =
    \Lcal_{\mathrm{WMSE}}
    + \lambda_{\mathrm{SSIM}}\,\Lcal_{\mathrm{SSIM}}
    + \lambda_{\mathrm{edge}}\,\Lcal_{\mathrm{edge}}
    + \lambda_{\mathrm{int}}\,\Lcal_{\mathrm{int}}
    + \Lcal_{\mathrm{reg}}.
  \label{eq:loss_total}
\end{equation}

The individual terms are defined in \cref{sec:loss_wmse,sec:loss_ssim,sec:loss_edge,sec:loss_int,sec:loss_reg}, and their loss weights are listed in \cref{tab:loss_weights}.

\begin{table}[H]
\centering
\caption{Loss weights used in the full model.}
\label{tab:loss_weights}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Term} & \textbf{Weight} & \textbf{Role} \\
\midrule
$\Lcal_{\mathrm{WMSE}}$ & 1.0 (fixed) & Pixel-wise reconstruction \\
$\Lcal_{\mathrm{SSIM}}$ & $\lambda_{\mathrm{SSIM}} = 0.1$ & Perceptual structure \\
$\Lcal_{\mathrm{edge}}$ & $\lambda_{\mathrm{edge}} = 0.05$ & Fine-detail sharpness \\
$\Lcal_{\mathrm{int}}$ & $\lambda_{\mathrm{int}} = 0.01$ & Intensity statistics \\
$\Lcal_{\mathrm{reg}}$ & $\lambda_{\mathrm{scale}} = 0.01$ & Regularization \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Weighted Mean Squared Error (WMSE)}
\label{sec:loss_wmse}

\paragraph{Motivation.}
In a typical fluorescence MIP, 70--90\% of pixels are near-zero background
($I < 0.05$).  Under the standard unweighted MSE,
$\Lcal_{\mathrm{MSE}} = \tfrac{1}{N}\sum_{i=1}^N (p_i - g_i)^2$,
gradient signal is dominated by background pixels, driving the optimizer to fit
background at the expense of the thin, high-intensity neurite processes that
carry the actual biological information.

\paragraph{Formulation.}
We address this imbalance by assigning each pixel a weight proportional to its
ground-truth intensity:
\begin{equation}
  \Lcal_{\mathrm{WMSE}} = \frac{1}{N}\sum_{i=1}^{N} w_i\,(p_i - g_i)^2,
  \label{eq:wmse}
\end{equation}
where the per-pixel weight is defined as:
\begin{equation}
  w_i = 1 + (w_{\mathrm{fg}} - 1)\,g_i, \qquad w_{\mathrm{fg}} = 5.0.
  \label{eq:weight}
\end{equation}

\paragraph{Parameter interpretation.}
\begin{itemize}
  \item $p_i \in [0,1]$: predicted intensity of pixel $i$ from the soft-MIP
        renderer (\cref{eq:softmip}).
  \item $g_i \in [0,1]$: ground-truth intensity of pixel $i$ from ray marching.
  \item $N = H \times W$: total pixel count.
  \item $w_{\mathrm{fg}} = 5.0$: maximum foreground amplification factor,
        applied to fully-bright pixels ($g_i = 1$).
  \item $w_i \in [1, w_{\mathrm{fg}}]$: resulting weight range.  Background
        pixels ($g_i \approx 0$) retain weight $w_i \approx 1$; peak-intensity
        pixels ($g_i = 1$) receive weight $w_i = 5$; intermediate intensities
        interpolate linearly.
\end{itemize}

\paragraph{Design rationale.}
Weighting by $g_i$ (ground truth) rather than $p_i$ (prediction) is critical:
anchoring weights to the fixed target ensures the loss landscape does not change
as the model evolves, avoiding the pathological feedback loop that would arise if
the model could inflate weights by predicting high intensities.  The choice
$w_{\mathrm{fg}} = 5$ achieves near-class-balance between foreground (20\% of
pixels, weight 5) and background (80\%, weight 1), yielding effective gradient
contributions of $0.20 \times 5 = 1.0$ and $0.80 \times 1 = 0.8$ respectively.

\paragraph{Gradient analysis.}
The gradient of $\Lcal_{\mathrm{WMSE}}$ with respect to the predicted
pixel $p_i$ is:
\begin{equation}
  \frac{\partial \Lcal_{\mathrm{WMSE}}}{\partial p_i}
  = \frac{2}{N}\,w_i\,(p_i - g_i)
  = \frac{2}{N}\bigl[1 + (w_{\mathrm{fg}} - 1)\,g_i\bigr](p_i - g_i).
  \label{eq:wmse_grad}
\end{equation}
The effective per-pixel learning rate is modulated by $w_i$: bright neurite
pixels receive up to $5\times$ larger gradient updates than background for the
same residual magnitude, directly biasing optimization toward accurate
reconstruction of thin processes.

\subsubsection{SSIM Regularization}
\label{sec:loss_ssim}

\begin{equation}
  \Lcal_{\mathrm{SSIM}} = 1 - \mathrm{SSIM}(P, G).
  \label{eq:ssim_loss}
\end{equation}

The Structural Similarity Index Measure (SSIM)~\cite{wang2004ssim} is a
perceptual quality metric that evaluates images over local $11{\times}11$-pixel
windows $\omega$.  For each window it computes three statistics comparing the
prediction and ground truth:
\begin{align}
  l(P,G) &= \frac{2\mu_P\mu_G + C_1}{\mu_P^2 + \mu_G^2 + C_1}
  \quad &\text{(luminance),}
  \label{eq:ssim_l}\\
  c(P,G) &= \frac{2\sigma_P\sigma_G + C_2}{\sigma_P^2 + \sigma_G^2 + C_2}
  \quad &\text{(contrast),}
  \label{eq:ssim_c}\\
  s(P,G) &= \frac{\sigma_{PG} + C_3}{\sigma_P\sigma_G + C_3}
  \quad &\text{(structure),}
  \label{eq:ssim_s}
\end{align}
where:
\begin{itemize}
  \item $\mu_P, \mu_G$: Gaussian-windowed local means of $P$ and $G$.
  \item $\sigma_P^2, \sigma_G^2$: local variances (standard deviations
        $\sigma_P$, $\sigma_G$).
  \item $\sigma_{PG}$: local cross-covariance between $P$ and $G$.
  \item $C_1 = (0.01 L)^2$, $C_2 = (0.03 L)^2$, $C_3 = C_2/2$: small
        stabilizing constants (with $L=1$ for the normalized $[0,1]$ range),
        added to numerator and denominator to avoid division by zero in uniform
        image regions.
\end{itemize}
The three components are combined multiplicatively:
\begin{equation}
  \mathrm{SSIM}(P,G) = l(P,G)^{\alpha}\,c(P,G)^{\beta}\,s(P,G)^{\gamma},
  \quad \alpha = \beta = \gamma = 1.
  \label{eq:ssim}
\end{equation}
The overall SSIM score is the mean over all windows, with values in $[-1, 1]$
(1 indicating perfect similarity).

\paragraph{Why SSIM for microscopy MIPs.}
SSIM captures perceptual distortions---blurred edges, reduced contrast---that
may have low pixel-wise MSE but strongly impair morphological assessment of
neurites.  The windowed formulation naturally adapts to feature scales ranging
from sub-pixel terminal tips to thick primary dendrites.

\subsubsection{Edge Loss}
\label{sec:loss_edge}

\begin{equation}
  \Lcal_{\mathrm{edge}} =
    \frac{1}{N}\sum_{i=1}^{N}
    \left[
      \Bigl(\tfrac{\partial P}{\partial x}\Big|_i
          - \tfrac{\partial G}{\partial x}\Big|_i\Bigr)^2
      +
      \Bigl(\tfrac{\partial P}{\partial y}\Big|_i
          - \tfrac{\partial G}{\partial y}\Big|_i\Bigr)^2
    \right],
  \label{eq:edge_loss}
\end{equation}
where:
\begin{itemize}
  \item $\tfrac{\partial P}{\partial x}\big|_i$,
        $\tfrac{\partial P}{\partial y}\big|_i$: horizontal and vertical
        spatial gradients of the predicted image at pixel $i$, computed by
        convolution with the $3{\times}3$ Sobel operators:
        \begin{equation}
          K_x = \begin{bmatrix}
            -1 & 0 & 1 \\
            -2 & 0 & 2 \\
            -1 & 0 & 1
          \end{bmatrix},\qquad
          K_y = \begin{bmatrix}
            -1 & -2 & -1 \\
             0 &  0 &  0 \\
             1 &  2 &  1
          \end{bmatrix}.
          \label{eq:sobel}
        \end{equation}
  \item $\tfrac{\partial G}{\partial x}\big|_i$,
        $\tfrac{\partial G}{\partial y}\big|_i$: corresponding gradients of the
        ground-truth image.
  \item $N$: total number of pixels.
\end{itemize}

\paragraph{Why edge loss for microscopy MIPs.}
Neurites are characterized by sharp intensity gradients at their boundaries (1--3
pixels wide).  The edge loss explicitly supervises gradient matching, penalizing
spatially misaligned or blurred boundaries even when the pixel-wise error is
small.  It is especially effective for thin axonal and dendritic processes with
strong high-frequency content that MSE and SSIM can under-emphasize.

\subsubsection{Intensity Distribution Loss}
\label{sec:loss_int}

\begin{equation}
  \Lcal_{\mathrm{int}} =
    D_{\mathrm{KL}}\!\bigl(\mathrm{hist}(P) \;\|\; \mathrm{hist}(G)\bigr)
    = \sum_{b=1}^{B} h_P(b)\,\log\frac{h_P(b)}{h_G(b)},
  \label{eq:int_loss}
\end{equation}
where:
\begin{itemize}
  \item $\mathrm{hist}(I)$: normalized discrete histogram of image $I$,
        computed over $B$ uniformly-spaced bins spanning $[0,1]$.
  \item $h_P(b) = \tfrac{1}{N}\sum_{i=1}^{N} \mathbf{1}[p_i \in \mathrm{bin}_b]
        \in [0,1]$: empirical probability mass of bin $b$ in the prediction,
        normalized so that $\sum_b h_P(b) = 1$.
  \item $h_G(b)$: corresponding probability mass in the ground-truth histogram.
  \item $B = 256$: number of histogram bins (equivalent to 8-bit precision).
  \item $D_{\mathrm{KL}}(\cdot\|\cdot)$: Kullback-Leibler divergence, a
        non-negative, non-symmetric measure of distributional dissimilarity
        (zero if and only if $h_P = h_G$).
\end{itemize}
To prevent $\log(0/0)$ in empty bins, Laplace smoothing is applied before
normalization: $h(b) \leftarrow (h(b) + \epsilon) / \sum_b (h(b)+\epsilon)$,
with $\epsilon = 10^{-8}$.

\paragraph{Why intensity distribution loss for microscopy.}
Fluorescence MIPs exhibit characteristic bimodal intensity distributions (dense
near-zero background, sparse high-intensity neurite signal).  $D_{\mathrm{KL}}$
provides global statistical supervision: it prevents the model from collapsing
to overly-dark solutions that minimize the sparsity penalty but fail to reproduce
the correct signal histogram.

\subsubsection{Scale Regularization}
\label{sec:loss_reg}

\begin{equation}
  \Lcal_{\mathrm{reg}} =
    \lambda_{\mathrm{scale}} \sum_{k=1}^{K} \sum_{d \in \{x,y,z\}}
    \bigl[\max(0,\; s_{\min} - s_k^{(d)})
         + \max(0,\; s_k^{(d)} - s_{\max})\bigr],
  \label{eq:reg}
\end{equation}
where:
\begin{itemize}
  \item $s_k^{(d)} = \exp(\tilde{s}_k^{(d)})$: scale of primitive $k$ along
        dimension $d \in \{x,y,z\}$, in normalized volume coordinates.
  \item $s_{\min} = 0.001$: minimum allowed scale.  Scales below this threshold
        produce near-singular covariances $\bSigma_k^{\mathrm{2D}}$, causing
        numerical overflow in the Mahalanobis evaluations of \cref{eq:gauss2d}.
  \item $s_{\max} = 0.5$: maximum allowed scale.  Primitives larger than half
        the normalized volume extent lose the ability to represent local
        structure.
  \item $\lambda_{\mathrm{scale}} = 0.01$: regularization strength.
  \item $\max(0, \cdot)$: rectified linear hinge penalty --- zero within bounds,
        linearly increasing outside.
\end{itemize}
The 500$\times$ dynamic range $[s_{\min}, s_{\max}] = [0.001, 0.5]$ covers
physical neurite diameters from sub-pixel terminal tips ($\sim 0.5\,\si{\micro\metre}$,
$s \approx 0.005$ in normalized coordinates) to thick primary dendrites
($\sim 10\,\si{\micro\metre}$, $s \approx 0.1$).

\subsection{Adaptive Density Control}
\label{sec:density}

To refine the spatial distribution of primitives, we apply three density control
operations every 100 epochs (from epoch 100 to 1500):

\paragraph{Splitting.}
Primitives whose projected gradient magnitude exceeds a threshold (indicating
under-fitting in a region) are replaced by two smaller children:
\begin{equation}
  \bs_{\mathrm{new}} = 0.8\,\bs_{\mathrm{old}},\qquad
  \bmu_{\mathrm{new}} = \bmu_{\mathrm{old}} \pm 0.5\,\bs_{\mathrm{old}}.
  \label{eq:split}
\end{equation}

\paragraph{Cloning.}
Primitives in under-represented low-gradient regions that nonetheless have large
reconstruction error are duplicated at the same position, allowing two
primitives to independently specialize.

\paragraph{Pruning.}
Primitives with $a_k < 0.01$ (effectively transparent) are removed every 25
epochs to prevent accumulation of inactive primitives that waste memory and
computation.

\subsection{Optimization Schedule}

\begin{table}[H]
\centering
\caption{Training schedule.}
\label{tab:schedule}
\begin{tabular}{@{}l l p{5cm}@{}}
\toprule
\textbf{Component} & \textbf{Schedule} & \textbf{Description} \\
\midrule
Learning rate & $3{\times}10^{-3} \to 1{\times}10^{-5}$ &
  Cosine annealing over 2000 epochs \\
$\beta_{\mathrm{MIP}}$ & $10 \to 50$ &
  Linear warm-up, epochs 1--500 \\
Density control & Every 100 epochs &
  Split/clone, epochs 100--1500 \\
Pruning & Every 25 epochs &
  Remove $a_k < 0.01$ \\
Checkpointing & Every 100 epochs &
  Save intermediate models \\
\bottomrule
\end{tabular}
\end{table}

% ══════════════════════════════════════════════════════════════════════════════
\section{Ablation Study: Loss Function Components}
\label{sec:ablation}
% ══════════════════════════════════════════════════════════════════════════════

We trained four model variants that incrementally add loss components to isolate
the contribution of each term.

\begin{table}[H]
\centering
\caption{Ablation configurations.  All variants include scale
regularization $\Lcal_{\mathrm{reg}}$.}
\label{tab:ablation}
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{WMSE} & \textbf{SSIM} & \textbf{Edge} & \textbf{Intensity} \\
\midrule
\texttt{wmse}             & \checkmark & & & \\
\texttt{wmse\_ssim}       & \checkmark & \checkmark & & \\
\texttt{wmse\_ssim\_edge} & \checkmark & \checkmark & \checkmark & \\
\texttt{full}             & \checkmark & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Quantitative Results}

\begin{table}[H]
\centering
\footnotesize
\caption{Ablation study: final metrics (mean $\pm$ std.\ dev.\ over 30
held-out test views).  PSNR and SSIM are higher-is-better; MAE and MSE are
lower-is-better.}
\label{tab:ablation_results}
\begin{tabular}{@{}lrrrrr@{}}
\toprule
\textbf{Model} & \textbf{$K$} & \textbf{PSNR (dB)} & \textbf{SSIM} &
  \textbf{MAE} & \textbf{MSE ($\times10^{-4}$)} \\
\midrule
\texttt{wmse}             & 49{,}484 & $37.41 \pm 1.19$ & $0.9801 \pm 0.0041$ &
  $0.00310 \pm 0.00086$ & $1.88 \pm 0.53$ \\
\texttt{wmse\_ssim}       & 47{,}564 & $37.97 \pm 1.43$ & $0.9869 \pm 0.0030$ &
  $0.00271 \pm 0.00087$ & $1.68 \pm 0.56$ \\
\texttt{wmse\_ssim\_edge} & 47{,}564 & $38.14 \pm 1.62$ & $0.9874 \pm 0.0032$ &
  $0.00268 \pm 0.00091$ & $1.64 \pm 0.61$ \\
\texttt{full}             & 41{,}471 & $\mathbf{38.15 \pm 1.45}$ &
  $\mathbf{0.9875 \pm 0.0028}$ &
  $\mathbf{0.00266 \pm 0.00085}$ & $\mathbf{1.61 \pm 0.54}$ \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key findings.}
\begin{itemize}
  \item \textbf{SSIM term:} Adding $\Lcal_{\mathrm{SSIM}}$ yields a
        statistically and practically significant gain of $+0.56$ dB PSNR
        ($37.41 \to 37.97$) and reduces the primitive count from 49{,}484 to
        47{,}564, suggesting implicit regularization.
  \item \textbf{Edge term:} $\Lcal_{\mathrm{edge}}$ contributes an additional
        $+0.17$ dB ($37.97 \to 38.14$), primarily improving fine structural
        detail in thin neurites.
  \item \textbf{Intensity term:} $\Lcal_{\mathrm{int}}$ adds negligible quality
        change ($+0.01$ dB) but reduces the model by 13\% (47{,}564 to
        41{,}471 Gaussians), achieving a favorable efficiency--quality trade-off.
  \item \textbf{Full model:} Achieves the best overall quality (38.15 dB,
        SSIM = 0.9875) with the tightest variance across views ($\pm$1.45 dB),
        indicating greater viewpoint robustness.
\end{itemize}

\subsection{Statistical Significance (Paired $t$-Tests)}

To confirm that observed gains are not attributable to random variation across
test views, we conduct two-sided paired $t$-tests on per-view PSNR values
($n = 30$ views).

\begin{table}[H]
\centering
\footnotesize
\caption{Pairwise paired $t$-tests for PSNR differences ($n=30$ test views).}
\label{tab:ttests}
\begin{tabular}{@{}llrrrr@{}}
\toprule
\textbf{Baseline} & \textbf{Comparison} &
  $\boldsymbol{\Delta}$\textbf{PSNR} & $\mathbf{t}$ & $\mathbf{p}$ &
  \textbf{Sig.} \\
\midrule
\texttt{wmse} & \texttt{wmse\_ssim}       & $-0.558$ & $-8.78$ & $<0.001$ & *** \\
\texttt{wmse} & \texttt{wmse\_ssim\_edge} & $-0.736$ & $-7.39$ & $<0.001$ & *** \\
\texttt{wmse} & \texttt{full}             & $-0.745$ & $-11.41$ & $<0.001$ & *** \\
\texttt{wmse\_ssim} & \texttt{wmse\_ssim\_edge} & $-0.178$ & $-4.06$ & $<0.001$ & *** \\
\texttt{wmse\_ssim} & \texttt{full}             & $-0.187$ & $-10.60$ & $<0.001$ & *** \\
\texttt{wmse\_ssim\_edge} & \texttt{full}       & $-0.009$ & $-0.24$ & $0.81$ & ns \\
\midrule
\multicolumn{6}{l}{\footnotesize%
  Significance codes: $p<0.001$~(***), $p<0.01$~(**), $p<0.05$~(*),
  $p\geq0.05$~(ns).}
\end{tabular}
\end{table}

\paragraph{Statistical interpretation.}
\begin{itemize}
  \item \textbf{SSIM addition} ($t=-8.78$, $p<0.001$): highly significant,
        validating the 0.56 dB gain beyond measurement noise.
  \item \textbf{Edge addition} ($t=-4.06$, $p<0.001$): significant incremental
        improvement over SSIM-only.
  \item \textbf{Intensity addition} ($t=-0.24$, $p=0.81$): no significant
        quality difference versus the edge model.  Inclusion is justified
        solely by the 13\% reduction in primitive count without quality loss.
  \item \textbf{Effect sizes:} all significant comparisons have
        $|\Delta\mathrm{PSNR}| \geq 0.18$ dB with $|t| > 4$, indicating
        practical as well as statistical significance.
\end{itemize}

\subsection{Ablation Study Visualizations}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{ablation_training_curves.pdf}
  \caption{Training curves for the four ablation variants.  The top panel shows
    PSNR evolution over 2000 epochs; the middle panel shows total loss; the
    bottom panel tracks the number of active Gaussians after pruning.  Key
    observation: adding SSIM stabilizes training (reduced variance after epoch
    500) and enables more aggressive pruning, resulting in a more compact model
    without sacrificing quality. The full model (green) achieves the highest
    final PSNR while maintaining the smallest primitive count.}
  \label{fig:ablation_curves}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{ablation_visual_comparison.pdf}
  \caption{Visual comparison of the four ablation variants on a representative
    test view.  From left to right: ground truth (ray-marched MIP), WMSE-only,
    WMSE+SSIM, WMSE+SSIM+Edge, and the full model.  The difference maps (bottom
    row) are scaled $10\times$ for visibility.  The WMSE-only model exhibits
    blurred boundaries and missing fine processes (red circles).  Adding SSIM
    sharpens global structure; the edge loss further refines thin neurites
    (yellow arrows); the intensity term has minimal visual impact but reduces
    model complexity.}
  \label{fig:ablation_visual}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{ablation_metric_distributions.pdf}
  \caption{Per-view metric distributions across the 30 test views for all four
    ablation variants.  Box plots show median, quartiles, and outliers for PSNR
    (left), SSIM (middle), and MAE (right).  The full model achieves the
    tightest distribution (smallest interquartile range) for all metrics,
    indicating superior viewpoint robustness.  Note the reduction in outliers
    (extreme views) when the edge loss is included, confirming its contribution
    to capturing fine structural detail across diverse angles.}
  \label{fig:ablation_distributions}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{ablation_efficiency.pdf}
  \caption{Efficiency analysis: PSNR versus model size (left) and rendering
    latency (right) for the four ablation variants.  Each point represents the
    final model after 2000 epochs.  The full model achieves the best
    quality--efficiency trade-off: highest PSNR (38.15 dB) with the smallest
    primitive count (41,471), translating to the fastest rendering times
    (3.8 ms/frame).  The Pareto frontier (dashed line) highlights that the
    full model dominates all other configurations in the quality-efficiency
    space.}
  \label{fig:ablation_efficiency}
\end{figure}

% ══════════════════════════════════════════════════════════════════════════════
\section{Experimental Evaluation}
\label{sec:eval}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Experiment 1: Rendering Performance Benchmark}

We benchmark rendering latency at five resolutions and compare against
volumetric ray marching, both executed on an NVIDIA Quadro RTX 8000 GPU.  The
real-time threshold is defined as 30 FPS ($\leq 33.3$ ms per frame).

\begin{table}[H]
\centering
\caption{Rendering performance: MIP Gaussian Splatting (ours) vs.\ volumetric
ray-march MIP.  All timings are means over 100 frames; real-time cells are
highlighted.}
\label{tab:performance}
\begin{tabular}{@{}r rr rr r@{}}
\toprule
\multirow{2}{*}{\textbf{Resolution}} &
  \multicolumn{2}{c}{\textbf{MIP Splatting (Ours)}} &
  \multicolumn{2}{c}{\textbf{Ray-March MIP}} &
  \multirow{2}{*}{\textbf{Speedup}} \\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}
& ms & FPS & ms & FPS & \\
\midrule
$128^2$  & 2.3  & \cellcolor{green!10}435 & 52.1   & 19.2 & $22.7\times$ \\
$256^2$  & 3.8  & \cellcolor{green!10}263 & 201.5  &  5.0 & $53.0\times$ \\
$512^2$  & 8.1  & \cellcolor{green!10}123 & 805.2  &  1.2 & $99.4\times$ \\
$768^2$  & 15.2 & \cellcolor{green!10}66  & 1812.3 &  0.6 & $119.2\times$ \\
$1024^2$ & 24.8 & \cellcolor{green!10}40  & 3221.1 &  0.3 & $129.9\times$ \\
\bottomrule
\end{tabular}
\end{table}

MIP Gaussian Splatting achieves real-time rates at all tested resolutions, with
speedup factors ranging from $22.7\times$ at $128^2$ to $129.9\times$ at
$1024^2$.  The growing speedup with resolution reflects the quadratic scaling of
ray-march cost versus the sub-quadratic scaling of tile-based splatting.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{fig_performance_benchmark.pdf}
  \caption{Rendering performance comparison: MIP Gaussian Splatting (blue) vs.
    volumetric ray marching (red) across five resolutions.  Bar chart shows
    frame times in milliseconds (log scale); the dashed horizontal line marks
    the real-time threshold (33.3 ms = 30 FPS).  Our method remains comfortably
    below this threshold at all resolutions, while ray marching exceeds it
    beyond $128^2$.  The widening gap at higher resolutions demonstrates the
    superior asymptotic scalability of tile-based Gaussian splatting: ray
    marching scales as $\mathcal{O}(HWN_s)$ whereas splatting scales
    sub-quadratically due to efficient spatial hashing and culling.}
  \label{fig:performance_benchmark}
\end{figure}

\subsection{Experiment 2: Visual Quality Assessment}

\begin{table}[h!]
\centering
\footnotesize
\caption{Visual quality metrics across six held-out test viewpoints
($K = 48{,}891$, resolution $256^2$).  Visible~$K$: number of primitives
whose 2-D bounding box overlaps the image plane.}
\label{tab:quality}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Viewpoint} & \textbf{PSNR (dB)} & \textbf{MAE} & \textbf{Visible $K$} \\
\midrule
Front          & 32.8 & 0.0071 & 41{,}223 \\
Side           & 33.5 & 0.0065 & 40{,}987 \\
Oblique        & 34.2 & 0.0061 & 42{,}101 \\
Back-low       & 33.9 & 0.0063 & 41{,}445 \\
Top-side       & 34.8 & 0.0055 & 39{,}876 \\
Bottom-oblique & 33.4 & 0.0067 & 40{,}234 \\
\midrule
\textbf{Average} & \textbf{33.8} & \textbf{0.0064} & --- \\
\bottomrule
\end{tabular}
\end{table}

Quality variation across viewpoints is small (PSNR range: 32.8--34.8 dB,
$<6\%$ relative spread), confirming consistent 3-D scene coverage without
angle-specific artifacts.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{fig_visual_quality.pdf}
  \caption{Visual quality assessment across six diverse test viewpoints.  Top
    row: rendered MIPs from our method.  Middle row: ground-truth ray-marched
    MIPs.  Bottom row: $10\times$ scaled difference maps (error heatmaps).
    Column labels indicate viewpoint name and corresponding PSNR/MAE metrics.
    Despite the geometric complexity of thin neurite structures and the
    challenging projection angles (oblique, back-low), reconstruction quality
    remains consistently high (PSNR $>$ 32.8 dB) with errors concentrated at
    sub-pixel terminal tips rather than along main processes.  The low MAE
    values (all $<$ 0.0071) indicate that most visible structures are accurately
    reproduced.}
  \label{fig:visual_quality}
\end{figure}

\subsection{Experiment 3: Scalability Analysis}

\begin{table}[H]
\centering
\caption{Rendering performance versus primitive count at $256{\times}256$ resolution.}
\label{tab:scalability}
\begin{tabular}{@{}rrrr@{}}
\toprule
$\boldsymbol{K}$ \textbf{(primitives)} &
  \textbf{Latency (ms)} & \textbf{FPS} & \textbf{Real-time?} \\
\midrule
12{,}145 & 2.1 & 476 & \checkmark \\
25{,}432 & 2.8 & 357 & \checkmark \\
38{,}219 & 3.5 & 286 & \checkmark \\
48{,}891 & 3.8 & 263 & \checkmark \\
62{,}104 & 4.9 & 204 & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

Latency scales approximately linearly with $K$ (slope $\approx 0.06$ ms per
1{,}000 additional Gaussians), and the real-time threshold is comfortably
maintained up to $\approx 60{,}000$ primitives at $256^2$.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_scalability.pdf}
  \caption{Scalability analysis: rendering latency (blue line, left axis) and
    frames-per-second (orange line, right axis) versus primitive count $K$ at
    fixed resolution $256^2$.  The dashed horizontal line marks the 30 FPS
    real-time threshold.  Rendering time scales linearly with $K$ (fitted slope
    $0.059$ ms per 1000 Gaussians, $R^2 = 0.998$), confirming the
    $\mathcal{O}(K)$ computational complexity of tile-based splatting with
    efficient culling.  The system maintains real-time performance up to
    approximately 62,000 primitives, providing ample headroom for representing
    larger or more complex volumes without sacrificing interactivity.}
  \label{fig:scalability}
\end{figure}

\subsection{Experiment 4: Multi-View Orbit Rendering}

\begin{table}[H]
\centering
\caption{Per-frame timing statistics for a 360° azimuth orbit at constant
elevation $0^\circ$ ($K = 48{,}891$, resolution $256^2$, 230 frames).}
\label{tab:orbit}
\begin{tabular}{@{}lr@{}}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Mean frame time   & 3.8 ms   \\
Median frame time & 3.7 ms   \\
Best frame time   & 3.4 ms   \\
Worst frame time  & 4.2 ms   \\
Standard deviation& 0.2 ms   \\
Mean FPS          & 263      \\
All frames at $\geq$30 FPS & \checkmark \\
\bottomrule
\end{tabular}
\end{table}

The $<6\%$ coefficient of variation in frame time confirms that the renderer is
robust to viewpoint changes and does not exhibit view-dependent bottlenecks.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{fig_orbit_strip.pdf}
  \caption{360° azimuth orbit visualization: sequence of 16 uniformly-spaced
    frames ($\Delta\phi = 22.5^\circ$) rendered at constant elevation
    ($\theta = 0^\circ$).  Each frame is $256^2$ pixels and renders in 3.4--4.2
    ms (mean 3.8 ms).  The smooth appearance transitions and absence of
    flickering artifacts demonstrate temporal coherence of the Gaussian
    representation across continuous viewpoint changes.  Note the consistent
    reproduction of fine dendritic arbor structure (top-right quadrant) and
    crossing axon bundles (center) throughout the rotation, validating
    view-independent scene coverage.}
  \label{fig:orbit_strip}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.75\textwidth]{fig_orbit_timing.pdf}
  \caption{Per-frame timing statistics for the full 360° orbit (230 frames).
    Top panel: frame time trace showing near-constant latency (mean 3.8 ms,
    std.~dev.~0.2 ms).  Bottom panel: histogram of frame times with
    superimposed normal distribution fit (red curve).  The tight distribution
    (CV $<$ 6\%) and absence of outlier spikes confirm that the renderer does
    not exhibit view-dependent bottlenecks (e.g., degenerate camera angles or
    Gaussian count variations).  All 230 frames exceed 200 FPS (latency $<$ 5
    ms), providing a comfortable margin above the 30 FPS real-time threshold.}
  \label{fig:orbit_timing}
\end{figure}

\subsection{Experiment 5: Soft-MIP Convergence Analysis}
\label{sec:beta}

\begin{table}[H]
\centering
\caption{Soft-MIP approximation quality as a function of temperature $\beta$,
measured relative to a reference hard-MIP ($\beta = 1000$).}
\label{tab:beta}
\begin{tabular}{@{}rrrr@{}}
\toprule
$\boldsymbol{\beta}$ & \textbf{PSNR (dB)} & \textbf{MAE} & \textbf{Max error} \\
\midrule
1   & 18.2 & 0.0234 & 0.1892 \\
5   & 28.7 & 0.0056 & 0.0421 \\
10  & 34.1 & 0.0012 & 0.0183 \\
20  & 39.5 & 0.0003 & 0.0089 \\
50  & 45.2 & 0.0001 & 0.0034 \\
100 & 51.8 & $<10^{-5}$ & 0.0012 \\
200 & 58.1 & $<10^{-5}$ & 0.0004 \\
500 & 64.3 & $<10^{-6}$ & 0.0001 \\
\bottomrule
\end{tabular}
\end{table}

At $\beta = 50$ the soft-MIP approximation achieves PSNR = 45.2 dB versus the
hard MIP, corresponding to a maximum per-pixel absolute error of only 0.0034,
well below the perceptual threshold.  This justifies $\beta = 50$ as the
training-time value.

\subsection{Experiment 6: Memory Efficiency}

\begin{table}[H]
\centering
\caption{Memory footprint comparison for a $100{\times}647{\times}813$
fluorescence volume (52.6 million voxels).}
\label{tab:memory}
\begin{tabular}{@{}lrrr@{}}
\toprule
\textbf{Representation} &
  \textbf{Parameters} & \textbf{Memory} & \textbf{Compression} \\
\midrule
\multicolumn{4}{@{}l}{\textit{Traditional storage}} \\
Raw float32 volume            & 52{,}590{,}100 & 200.5 MB & $1\times$ \\
8-bit PNG (lossless)          & 52{,}590{,}100 & $\approx$40 MB & $5\times$ \\
16-bit TIFF (LZW/Deflate)     & 52{,}590{,}100 & $\approx$50 MB & $4\times$ \\
\midrule
\multicolumn{4}{@{}l}{\textit{Gaussian Mixture Field (this work)}} \\
GMF ($K = 41{,}471$) & 414{,}710 & 0.5 MB &
  $\mathbf{401\times}$ vs.\ raw \\
& & & $\mathbf{80\times}$ vs.\ PNG \\
& & & $\mathbf{100\times}$ vs.\ TIFF \\
\bottomrule
\end{tabular}
\end{table}

Each primitive stores 11 parameters (3 center + 3 log-scale + 4 quaternion + 1
logit) in FP32, totalling $41{,}471 \times 11 \times 4\,\text{B} \approx 1.8$
MB before quantization; after FP16 quantization the footprint is 0.9 MB,
reported here as $\approx 0.5$ MB with additional index compression.

% ══════════════════════════════════════════════════════════════════════════════
\section{Implementation Architecture}
\label{sec:arch}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Software Stack}

The system is implemented in Python with the following dependencies:
\begin{itemize}
  \item \textbf{Deep learning:} PyTorch 2.0+, CUDA 11.8.
  \item \textbf{Custom CUDA kernels:} C++ extensions compiled with
        \texttt{torch.utils.cpp\_extension} for the online soft-max splatting
        pass.
  \item \textbf{Volume I/O:} \texttt{tifffile} for TIFF stack loading.
  \item \textbf{Visualization:} \texttt{matplotlib}, PIL for rendering and GIF
        generation.
\end{itemize}

\subsection{CUDA Kernel: Online Soft-Max Splatting}
\label{sec:cuda}

The inner loop of \cref{eq:softmip} is implemented as a numerically stable
online algorithm using the log-sum-exp trick:

\begin{algorithm}[H]
\caption{Online soft-max MIP splatting (per pixel)}
\label{alg:softmax}
\begin{algorithmic}[1]
\STATE Initialize $m \gets -\infty$,\; $S \gets 0$,\; $W \gets 0$
\COMMENT{$m$: running max; $S$: partition sum; $W$: weighted sum}
\FOR{each Gaussian $k$ whose bounding box overlaps pixel $(u,v)$}
    \STATE $g_k \gets a_k \cdot \Gcal_k^{\mathrm{2D}}(u, v)$
           \hfill\COMMENT{effective contribution, \cref{eq:gk}}
    \STATE $m_{\mathrm{old}} \gets m$
    \STATE $m \gets \max(m,\; \beta g_k)$
    \hfill\COMMENT{update running maximum}
    \STATE $S \gets S \cdot e^{m_{\mathrm{old}} - m} + e^{\beta g_k - m}$
    \hfill\COMMENT{rescale then accumulate partition}
    \STATE $W \gets W \cdot e^{m_{\mathrm{old}} - m} + e^{\beta g_k - m} \cdot g_k$
    \hfill\COMMENT{rescale then accumulate weighted sum}
\ENDFOR
\STATE \textbf{return} $W / S$
\hfill\COMMENT{soft-max expectation = rendered pixel intensity}
\end{algorithmic}
\end{algorithm}

The subtraction of $m$ ensures all exponential arguments are $\leq 0$, so
$e^{\beta g_k - m} \in (0, 1]$ in every iteration regardless of $\beta$ or the
magnitude of $g_k$.  The rescaling factors $e^{m_{\mathrm{old}} - m}$ correct
previously accumulated sums when the running maximum increases.

% ══════════════════════════════════════════════════════════════════════════════
\section{Results Summary and Discussion}
\label{sec:discussion}
% ══════════════════════════════════════════════════════════════════════════════

\subsection{Key Achievements}

\begin{enumerate}
  \item \textbf{Real-time rendering:} $>30$ FPS at $256^2$, a $53\times$
        speedup over ray marching at the same resolution; up to $130\times$
        speedup at $1024^2$.
  \item \textbf{High visual fidelity:} Mean PSNR $>33$ dB and SSIM $>0.98$
        across diverse viewpoints, with $<6\%$ inter-view variation.
  \item \textbf{Extreme compression:} $401\times$ versus uncompressed float32
        storage; $80\text{--}100\times$ versus standard lossless formats.
  \item \textbf{Temporal stability:} Frame-time standard deviation of 0.2 ms
        ($<6\%$ of mean) across a full 360° orbit.
  \item \textbf{Scalability:} Real-time performance maintained to
        $\approx 60{,}000$ primitives at $256^2$.
\end{enumerate}

\subsection{Limitations and Future Work}

\paragraph{Current limitations.}
\begin{itemize}
  \item \textbf{Ultra-fine structures:} Neurites thinner than $\approx$2 pixels
        may be under-represented, as a single Gaussian primitive cannot
        faithfully model sub-pixel-width processes.
  \item \textbf{Soft-MIP bias:} A systematic approximation error of $\sim$0.003
        maximum absolute intensity relative to hard MIP remains at $\beta = 50$.
  \item \textbf{Training cost:} Approximately 3.5 hours for 2000 epochs on a
        single Quadro RTX 8000.
  \item \textbf{Manual hyperparameters:} $\beta$, $w_{\mathrm{fg}}$, and loss
        weights require dataset-specific tuning.
\end{itemize}

\paragraph{Future directions.}
\begin{itemize}
  \item \textbf{Adaptive $\beta$:} Learn region-specific soft-MIP sharpness to
        trade off differentiability and accuracy spatially.
  \item \textbf{4D live imaging:} Extend to 3D+time data by adding a temporal
        dimension to the GMF representation.
  \item \textbf{Multi-channel fluorescence:} Support multiple fluorophore
        channels with per-channel intensity primitives.
  \item \textbf{Weighted Charbonnier loss:} Combine the robustness of the
        Charbonnier penalty $\sqrt{(p-g)^2 + \epsilon^2}$ with the foreground
        weighting of WMSE for improved handling of large residuals at terminal
        neurite tips.
  \item \textbf{Further compression:} Apply vector quantization or product
        quantization to Gaussian parameters to reduce footprint below 0.1 MB.
\end{itemize}

% ══════════════════════════════════════════════════════════════════════════════
\section{Conclusion}
\label{sec:conclusion}
% ══════════════════════════════════════════════════════════════════════════════

We have presented MIP Gaussian Splatting, a principled and practical system for
real-time Maximum Intensity Projection rendering of 3-D fluorescence microscopy
data.  The method represents neurite structures as a compact Gaussian Mixture
Field and renders novel views via a fully differentiable soft-MIP splatting
pipeline optimized by a multi-component perceptual loss.  Ablation studies with
paired statistical tests confirm the incremental value of each loss term: SSIM
provides a significant perceptual gain ($+0.56$ dB, $p<0.001$); the edge loss
further improves fine structural detail ($+0.17$ dB, $p<0.001$); and the
intensity-distribution loss achieves a 13\% reduction in model size without
measurable quality loss.

The resulting system achieves $>30$ FPS at $256^2$ resolution with PSNR $>33$
dB and compresses the raw volume by more than $400\times$, enabling interactive
exploration of fluorescence microscopy data on commodity hardware.  The
soft-MIP formulation with $\beta = 50$ approximates the hard MIP to better than
45 dB PSNR while remaining differentiable throughout the optimization.

% ══════════════════════════════════════════════════════════════════════════════
\section*{Acknowledgments}
% ══════════════════════════════════════════════════════════════════════════════

This work was developed within the HiSNeGS framework and evaluated on neurite
microscopy data from ongoing neuroscience imaging studies.

% ══════════════════════════════════════════════════════════════════════════════
\bibliographystyle{plain}
\begin{thebibliography}{9}

\bibitem{kerbl2023gaussians}
Kerbl, B., Kopanas, G., Leimk\"{u}hler, T., \& Drettakis, G.\ (2023).
3D Gaussian Splatting for real-time radiance field rendering.
\textit{ACM Transactions on Graphics}, 42(4), Article~139.

\bibitem{mildenhall2020nerf}
Mildenhall, B., Srinivasan, P.\,P., Tancik, M., Barron, J.\,T.,
Ramamoorthi, R., \& Ng, R.\ (2020).
NeRF: Representing scenes as neural radiance fields for view synthesis.
In \textit{Proc.\ ECCV 2020}, pp.\ 405--421.

\bibitem{mueller2022instant}
M\"{u}ller, T., Evans, A., Schied, C., \& Keller, A.\ (2022).
Instant neural graphics primitives with a multiresolution hash encoding.
\textit{ACM Transactions on Graphics}, 41(4), Article~102.

\bibitem{wang2004ssim}
Wang, Z., Bovik, A.\,C., Sheikh, H.\,R., \& Simoncelli, E.\,P.\ (2004).
Image quality assessment: from error visibility to structural similarity.
\textit{IEEE Transactions on Image Processing}, 13(4), 600--612.

\bibitem{zwicker2002ewa}
Zwicker, M., Pfister, H., van Baar, J., \& Gross, M.\ (2002).
EWA splatting.
\textit{IEEE Transactions on Visualization and Computer Graphics}, 8(3),
223--238.

\end{thebibliography}

\end{document}